{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original to Mapped Category IDs:\n",
      "Original ID: 0 -> Mapped ID: 1\n",
      "Original ID: 1 -> Mapped ID: 2\n",
      "Original ID: 2 -> Mapped ID: 3\n",
      "Original ID: 3 -> Mapped ID: 4\n",
      "Original ID: 4 -> Mapped ID: 5\n",
      "Original ID: 5 -> Mapped ID: 6\n",
      "Original ID: 6 -> Mapped ID: 7\n",
      "Original ID: 7 -> Mapped ID: 8\n",
      "Original ID: 8 -> Mapped ID: 9\n",
      "Original ID: 9 -> Mapped ID: 10\n",
      "Original ID: 10 -> Mapped ID: 11\n",
      "Original ID: 11 -> Mapped ID: 13\n",
      "Original ID: 12 -> Mapped ID: 14\n",
      "Original ID: 13 -> Mapped ID: 15\n",
      "Original ID: 14 -> Mapped ID: 16\n",
      "Original ID: 15 -> Mapped ID: 17\n",
      "Original ID: 16 -> Mapped ID: 18\n",
      "Original ID: 17 -> Mapped ID: 19\n",
      "Original ID: 18 -> Mapped ID: 20\n",
      "Original ID: 19 -> Mapped ID: 21\n",
      "Original ID: 20 -> Mapped ID: 22\n",
      "Original ID: 21 -> Mapped ID: 23\n",
      "Original ID: 22 -> Mapped ID: 24\n",
      "Original ID: 23 -> Mapped ID: 25\n",
      "Original ID: 24 -> Mapped ID: 27\n",
      "Original ID: 25 -> Mapped ID: 28\n",
      "Original ID: 26 -> Mapped ID: 31\n",
      "Original ID: 27 -> Mapped ID: 32\n",
      "Original ID: 28 -> Mapped ID: 33\n",
      "Original ID: 29 -> Mapped ID: 34\n",
      "Original ID: 30 -> Mapped ID: 35\n",
      "Original ID: 31 -> Mapped ID: 36\n",
      "Original ID: 32 -> Mapped ID: 37\n",
      "Original ID: 33 -> Mapped ID: 38\n",
      "Original ID: 34 -> Mapped ID: 39\n",
      "Original ID: 35 -> Mapped ID: 40\n",
      "Original ID: 36 -> Mapped ID: 41\n",
      "Original ID: 37 -> Mapped ID: 42\n",
      "Original ID: 38 -> Mapped ID: 43\n",
      "Original ID: 39 -> Mapped ID: 44\n",
      "Original ID: 40 -> Mapped ID: 46\n",
      "Original ID: 41 -> Mapped ID: 47\n",
      "Original ID: 42 -> Mapped ID: 48\n",
      "Original ID: 43 -> Mapped ID: 49\n",
      "Original ID: 44 -> Mapped ID: 50\n",
      "Original ID: 45 -> Mapped ID: 51\n",
      "Original ID: 46 -> Mapped ID: 52\n",
      "Original ID: 47 -> Mapped ID: 53\n",
      "Original ID: 48 -> Mapped ID: 54\n",
      "Original ID: 49 -> Mapped ID: 55\n",
      "Original ID: 50 -> Mapped ID: 56\n",
      "Original ID: 51 -> Mapped ID: 57\n",
      "Original ID: 52 -> Mapped ID: 58\n",
      "Original ID: 53 -> Mapped ID: 59\n",
      "Original ID: 54 -> Mapped ID: 60\n",
      "Original ID: 55 -> Mapped ID: 61\n",
      "Original ID: 56 -> Mapped ID: 62\n",
      "Original ID: 57 -> Mapped ID: 63\n",
      "Original ID: 58 -> Mapped ID: 64\n",
      "Original ID: 59 -> Mapped ID: 65\n",
      "Original ID: 60 -> Mapped ID: 67\n",
      "Original ID: 61 -> Mapped ID: 70\n",
      "Original ID: 62 -> Mapped ID: 72\n",
      "Original ID: 63 -> Mapped ID: 73\n",
      "Original ID: 64 -> Mapped ID: 74\n",
      "Original ID: 65 -> Mapped ID: 75\n",
      "Original ID: 66 -> Mapped ID: 76\n",
      "Original ID: 67 -> Mapped ID: 77\n",
      "Original ID: 68 -> Mapped ID: 78\n",
      "Original ID: 69 -> Mapped ID: 79\n",
      "Original ID: 70 -> Mapped ID: 80\n",
      "Original ID: 71 -> Mapped ID: 81\n",
      "Original ID: 72 -> Mapped ID: 82\n",
      "Original ID: 73 -> Mapped ID: 84\n",
      "Original ID: 74 -> Mapped ID: 85\n",
      "Original ID: 75 -> Mapped ID: 86\n",
      "Original ID: 76 -> Mapped ID: 87\n",
      "Original ID: 77 -> Mapped ID: 88\n",
      "Original ID: 78 -> Mapped ID: 89\n",
      "Original ID: 79 -> Mapped ID: 90\n"
     ]
    }
   ],
   "source": [
    "# Load the annotation file\n",
    "annotation_file = 'data/COCO/annotations/instances_val2017.json'  # Replace with your annotation file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract original category IDs and names\n",
    "categories = coco_data['categories']\n",
    "original_to_mapped = {category['id']: idx for idx, category in enumerate(categories)}\n",
    "mapped_to_original = {v: k for k, v in original_to_mapped.items()}\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Original to Mapped Category IDs:\")\n",
    "for orig_id, new_id in mapped_to_original.items():\n",
    "    print(f\"Original ID: {orig_id} -> Mapped ID: {new_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_features, max_bboxes=10, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())  # List of image IDs\n",
    "        self.transform = transform\n",
    "        self.image_features = image_features\n",
    "        self.max_bboxes = max_bboxes\n",
    "        self.num_classes = 80  # COCO has 80 categories, adjust if needed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def pad_bounding_boxes(self, bbox):\n",
    "        \"\"\"\n",
    "        Pad or truncate bounding boxes to a fixed size.\n",
    "        \"\"\"\n",
    "        bbox_tensor = torch.tensor(bbox, dtype=torch.float32)\n",
    "        num_bboxes = bbox_tensor.size(0)\n",
    "\n",
    "        if num_bboxes > self.max_bboxes:\n",
    "            # Truncate bounding boxes\n",
    "            bbox_tensor = bbox_tensor[:self.max_bboxes]\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            pad_size = self.max_bboxes - num_bboxes\n",
    "            padding = torch.zeros((pad_size, bbox_tensor.size(1)))\n",
    "            bbox_tensor = torch.cat([bbox_tensor, padding], dim=0)\n",
    "\n",
    "        return bbox_tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image ID\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(annotations)\n",
    "        \n",
    "        # Initialize an empty mask (will be filled in with object masks), bounding boxes, and category counts\n",
    "        mask = np.zeros((image.size[1], image.size[0]), dtype=np.float32)  # HxW for mask\n",
    "        bounding_boxes = []\n",
    "        feature_vector = []\n",
    "        category_counts = np.zeros(self.num_classes, dtype=np.float32)  # For counting the number of objects per class\n",
    "\n",
    "        # Loop through annotations and add segmentation masks, bounding boxes, and category counts\n",
    "        for ann in annotations:\n",
    "            if 'segmentation' in ann:\n",
    "                ann_mask = self.coco.annToMask(ann)\n",
    "                mask = np.maximum(mask, ann_mask)  # Combine multiple masks (take max)\n",
    "            # Bounding boxes\n",
    "            if 'bbox' in ann:\n",
    "                bbox = ann['bbox']  # [x, y, width, height]\n",
    "                bounding_boxes.append(bbox)\n",
    "            # Image features\n",
    "            if 'coco_url' in ann: \n",
    "                feature_vector = self.image_features.get(ann['coco_url'], np.zeros(2048))  # Adjust default value if needed\n",
    "            \n",
    "            # Count the category\n",
    "            category_id = ann['category_id']\n",
    "            category_counts[original_to_mapped[category_id]] += 1  # Categories are 1-indexed in COCO, so subtract 1\n",
    "        \n",
    "        # Pad/truncate bounding boxes\n",
    "        bounding_boxes = self.pad_bounding_boxes(bounding_boxes)\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image, mask, bounding_boxes = self.transform(image, mask, bounding_boxes)\n",
    "            \n",
    "        # Convert to tensor\n",
    "        feature_vector = torch.tensor(feature_vector, dtype=torch.float32)\n",
    "        category_counts = torch.tensor(category_counts, dtype=torch.float32)\n",
    "        bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
    "\n",
    "        return image, mask, bounding_boxes, feature_vector, category_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize image to match ResNet input size\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet statistics\n",
    "# ])\n",
    "\n",
    "class CustomTransform:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.transform_image = transforms.Compose([\n",
    "            transforms.Resize(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image, mask, bounding_boxes):\n",
    "        # Get the original dimensions of the image\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        image = self.transform_image(image)\n",
    "        \n",
    "        # Resize mask (ensure it's the same size as the image)\n",
    "        mask = Image.fromarray(mask)  # Convert mask to PIL Image for resizing\n",
    "        mask = mask.resize((self.size[1], self.size[0]), Image.NEAREST)  # Use nearest neighbor to preserve the mask\n",
    "        mask = np.array(mask)  # Convert back to NumPy array\n",
    "\n",
    "        # Resize bounding boxes based on the new image size\n",
    "        w_ratio = self.size[1] / float(original_width)  # Width resize ratio\n",
    "        h_ratio = self.size[0] / float(original_height)  # Height resize ratio\n",
    "        adjusted_bboxes = []\n",
    "        for bbox in bounding_boxes:\n",
    "            x, y, w, h = bbox\n",
    "            x *= w_ratio\n",
    "            y *= h_ratio\n",
    "            w *= w_ratio\n",
    "            h *= h_ratio\n",
    "            adjusted_bboxes.append([x, y, w, h])\n",
    "\n",
    "        # Convert mask to tensor\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        adjusted_bboxes = torch.tensor(adjusted_bboxes, dtype=torch.float32)\n",
    "\n",
    "        # Return the transformed image, mask, and bounding boxes\n",
    "        return image, mask, adjusted_bboxes\n",
    "\n",
    "\n",
    "transform = CustomTransform(size=(224, 224))  # Resize to 224x224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    }
   ],
   "source": [
    "data_type = \"train\"\n",
    "dataset = \"train2017\"\n",
    "\n",
    "# Set your paths\n",
    "train_images_dir = f'data/COCO/{dataset}'\n",
    "train_annotations_dir = f'data/COCO/annotations/instances_{dataset}.json'\n",
    "\n",
    "# Load image feature\n",
    "features_dir = os.path.join(\"data\",\"COCO\",\"extracted_features\",f'{data_type}_image_features.json')\n",
    "with open(features_dir, 'r') as f:\n",
    "    image_features = json.load(f)\n",
    "\n",
    "num_bboxes = 20\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = COCODataset(train_images_dir, train_annotations_dir,image_features,max_bboxes=num_bboxes, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 118287\n",
      "train_loader size: 3697\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset))\n",
    "print(\"train_loader size:\", len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, resnet_output_size=2048, num_bboxes=4, num_classes=80): \n",
    "        \"\"\"\n",
    "        Initialize the multi-modal model. \n",
    "        Args:\n",
    "        - resnet_output_size: The size of the ResNet feature vector (2048 for ResNet-50).\n",
    "        - num_bboxes: The number of bounding box entries (e.g., 4 for x, y, width, height).\n",
    "        - num_classes: The number of output classes (i.e., the number of categories to predict counts for).\n",
    "        \"\"\"\n",
    "        super(MultiModalModel, self).__init__()\n",
    "\n",
    "        # Image feature processing (ResNet features)\n",
    "        self.resnet_fc = nn.Linear(resnet_output_size, 512)\n",
    "\n",
    "        # Segmentation mask processing (using a simple CNN)\n",
    "        self.mask_conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.mask_conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.mask_fc = nn.Linear(128 * 64 * 64, 512)  # You may need to adjust the size depending on image resolution\n",
    "\n",
    "        # Bounding box processing (fully connected layer)\n",
    "        self.bbox_fc = nn.Linear(num_bboxes, 128)\n",
    "\n",
    "        # Final count output (instead of classification, we predict a count for each category)\n",
    "        self.fc_count = nn.Linear(512 + 512 + 128, num_classes)  # Output count for each class\n",
    "\n",
    "    def forward(self, image, mask, bbox, feature_vector, category_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-modal model\n",
    "        Args:\n",
    "        - image: Tensor of image data, shape [batch_size, channels, height, width]\n",
    "        - mask: Tensor of segmentation mask data, shape [batch_size, height, width]\n",
    "        - bbox: Tensor of bounding boxes, shape [batch_size, num_bboxes]\n",
    "        - feature_vector: Tensor of feature vectors from ResNet, shape [batch_size, 2048]\n",
    "        - category_ids: Tensor of category IDs, shape [batch_size, num_categories]\n",
    "        \"\"\"\n",
    "\n",
    "        # Process the ResNet feature vector\n",
    "        feature_vector_out = self.resnet_fc(feature_vector)  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Process the segmentation mask\n",
    "        mask_out = F.relu(self.mask_conv1(mask))  # Apply convolution\n",
    "        mask_out = F.relu(self.mask_conv2(mask_out))  # Apply second convolution\n",
    "        mask_out = mask_out.view(mask_out.size(0), -1)  # Flatten for FC layer\n",
    "        mask_out = self.mask_fc(mask_out)  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Process the bounding boxes\n",
    "        bbox_out = F.relu(self.bbox_fc(bbox))  # Shape: [batch_size, 128]\n",
    "\n",
    "        # Concatenate all the features\n",
    "        combined_features = torch.cat((feature_vector_out, mask_out, bbox_out), dim=1)  # Shape: [batch_size, 1152]\n",
    "\n",
    "        # Final count output (predicting the count for each category)\n",
    "        count_output = self.fc_count(combined_features)  # Shape: [batch_size, num_classes]\n",
    "        # Each value in count_output corresponds to the predicted count for a category\n",
    "\n",
    "        return count_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/3697 [00:00<?, ?batch/s]C:\\Users\\Moodswing\\AppData\\Local\\Temp\\ipykernel_14084\\1079901809.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
      "Epoch 1/10:   0%|          | 0/3697 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 4] at entry 0 and [2, 4] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Use tqdm to iterate through the DataLoader\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks, bounding_boxes, feature_vectors, category_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     15\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m     bounding_boxes \u001b[38;5;241m=\u001b[39m bounding_boxes\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 4] at entry 0 and [2, 4] at entry 2"
     ]
    }
   ],
   "source": [
    "# Initialize the model, optimizer, and loss functions\n",
    "num_epochs = 10\n",
    "model = MultiModalModel(resnet_output_size=2048, num_bboxes=num_bboxes, num_classes=80).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Loss function: Mean Squared Error for regression\n",
    "criterion_count = nn.MSELoss()  # For predicting counts\n",
    "\n",
    "# Training loop with tqdm for progress bars\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Use tqdm to iterate through the DataLoader\n",
    "    for images, masks, bounding_boxes, feature_vectors, category_ids in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        masks = masks.to(device)\n",
    "        bounding_boxes = bounding_boxes.to(device)\n",
    "        feature_vectors = feature_vectors.to(device)\n",
    "        category_ids = category_ids.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        count_output = model(masks, bounding_boxes, feature_vectors, category_ids)\n",
    "\n",
    "        # Calculate the count loss (regression)\n",
    "        loss_count = criterion_count(count_output, category_ids.float())  # category_ids should be the true count of each class\n",
    "        \n",
    "        # You may also calculate segmentation and bbox losses if you need them\n",
    "        # loss_segmentation = criterion_segmentation(segmentation_output, masks)\n",
    "        # loss_bbox = criterion_bbox(bbox_output, bounding_boxes)\n",
    "\n",
    "        # Total loss (fusion of regression losses)\n",
    "        total_loss = loss_count  # + loss_segmentation + loss_bbox (if necessary)\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss values for each epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Count Loss: {loss_count:.4f}, Total Loss: {total_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
