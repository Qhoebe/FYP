{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original to Mapped Category IDs:\n",
      "Original ID: 0 -> Mapped ID: 1\n",
      "Original ID: 1 -> Mapped ID: 2\n",
      "Original ID: 2 -> Mapped ID: 3\n",
      "Original ID: 3 -> Mapped ID: 4\n",
      "Original ID: 4 -> Mapped ID: 5\n",
      "Original ID: 5 -> Mapped ID: 6\n",
      "Original ID: 6 -> Mapped ID: 7\n",
      "Original ID: 7 -> Mapped ID: 8\n",
      "Original ID: 8 -> Mapped ID: 9\n",
      "Original ID: 9 -> Mapped ID: 10\n",
      "Original ID: 10 -> Mapped ID: 11\n",
      "Original ID: 11 -> Mapped ID: 13\n",
      "Original ID: 12 -> Mapped ID: 14\n",
      "Original ID: 13 -> Mapped ID: 15\n",
      "Original ID: 14 -> Mapped ID: 16\n",
      "Original ID: 15 -> Mapped ID: 17\n",
      "Original ID: 16 -> Mapped ID: 18\n",
      "Original ID: 17 -> Mapped ID: 19\n",
      "Original ID: 18 -> Mapped ID: 20\n",
      "Original ID: 19 -> Mapped ID: 21\n",
      "Original ID: 20 -> Mapped ID: 22\n",
      "Original ID: 21 -> Mapped ID: 23\n",
      "Original ID: 22 -> Mapped ID: 24\n",
      "Original ID: 23 -> Mapped ID: 25\n",
      "Original ID: 24 -> Mapped ID: 27\n",
      "Original ID: 25 -> Mapped ID: 28\n",
      "Original ID: 26 -> Mapped ID: 31\n",
      "Original ID: 27 -> Mapped ID: 32\n",
      "Original ID: 28 -> Mapped ID: 33\n",
      "Original ID: 29 -> Mapped ID: 34\n",
      "Original ID: 30 -> Mapped ID: 35\n",
      "Original ID: 31 -> Mapped ID: 36\n",
      "Original ID: 32 -> Mapped ID: 37\n",
      "Original ID: 33 -> Mapped ID: 38\n",
      "Original ID: 34 -> Mapped ID: 39\n",
      "Original ID: 35 -> Mapped ID: 40\n",
      "Original ID: 36 -> Mapped ID: 41\n",
      "Original ID: 37 -> Mapped ID: 42\n",
      "Original ID: 38 -> Mapped ID: 43\n",
      "Original ID: 39 -> Mapped ID: 44\n",
      "Original ID: 40 -> Mapped ID: 46\n",
      "Original ID: 41 -> Mapped ID: 47\n",
      "Original ID: 42 -> Mapped ID: 48\n",
      "Original ID: 43 -> Mapped ID: 49\n",
      "Original ID: 44 -> Mapped ID: 50\n",
      "Original ID: 45 -> Mapped ID: 51\n",
      "Original ID: 46 -> Mapped ID: 52\n",
      "Original ID: 47 -> Mapped ID: 53\n",
      "Original ID: 48 -> Mapped ID: 54\n",
      "Original ID: 49 -> Mapped ID: 55\n",
      "Original ID: 50 -> Mapped ID: 56\n",
      "Original ID: 51 -> Mapped ID: 57\n",
      "Original ID: 52 -> Mapped ID: 58\n",
      "Original ID: 53 -> Mapped ID: 59\n",
      "Original ID: 54 -> Mapped ID: 60\n",
      "Original ID: 55 -> Mapped ID: 61\n",
      "Original ID: 56 -> Mapped ID: 62\n",
      "Original ID: 57 -> Mapped ID: 63\n",
      "Original ID: 58 -> Mapped ID: 64\n",
      "Original ID: 59 -> Mapped ID: 65\n",
      "Original ID: 60 -> Mapped ID: 67\n",
      "Original ID: 61 -> Mapped ID: 70\n",
      "Original ID: 62 -> Mapped ID: 72\n",
      "Original ID: 63 -> Mapped ID: 73\n",
      "Original ID: 64 -> Mapped ID: 74\n",
      "Original ID: 65 -> Mapped ID: 75\n",
      "Original ID: 66 -> Mapped ID: 76\n",
      "Original ID: 67 -> Mapped ID: 77\n",
      "Original ID: 68 -> Mapped ID: 78\n",
      "Original ID: 69 -> Mapped ID: 79\n",
      "Original ID: 70 -> Mapped ID: 80\n",
      "Original ID: 71 -> Mapped ID: 81\n",
      "Original ID: 72 -> Mapped ID: 82\n",
      "Original ID: 73 -> Mapped ID: 84\n",
      "Original ID: 74 -> Mapped ID: 85\n",
      "Original ID: 75 -> Mapped ID: 86\n",
      "Original ID: 76 -> Mapped ID: 87\n",
      "Original ID: 77 -> Mapped ID: 88\n",
      "Original ID: 78 -> Mapped ID: 89\n",
      "Original ID: 79 -> Mapped ID: 90\n"
     ]
    }
   ],
   "source": [
    "# Load the annotation file\n",
    "annotation_file = 'data/COCO/annotations/instances_val2017.json'  # Replace with your annotation file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract original category IDs and names\n",
    "categories = coco_data['categories']\n",
    "original_to_mapped = {category['id']: idx for idx, category in enumerate(categories)}\n",
    "mapped_to_original = {v: k for k, v in original_to_mapped.items()}\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Original to Mapped Category IDs:\")\n",
    "for orig_id, new_id in mapped_to_original.items():\n",
    "    print(f\"Original ID: {orig_id} -> Mapped ID: {new_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_features, max_bboxes=10, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())  # List of image IDs\n",
    "        self.transform = transform\n",
    "        self.image_features = image_features\n",
    "        self.max_bboxes = max_bboxes\n",
    "        self.num_classes = 80  # COCO has 80 categories, adjust if needed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def pad_bounding_boxes(self, bbox):\n",
    "        \"\"\"\n",
    "        Pads the bounding boxes to have a fixed number of entries.\n",
    "\n",
    "        Args:\n",
    "        - bbox: List of bounding boxes, where each bounding box is [x, y, width, height].\n",
    "\n",
    "        Returns:\n",
    "        - Padded tensor of bounding boxes with shape [self.max_bboxes, 4].\n",
    "        \"\"\"\n",
    "        if len(bbox) == 0:  # If no bounding boxes are available\n",
    "            bbox_tensor = torch.zeros((self.max_bboxes, 4))  # Pad with zeros\n",
    "        else:\n",
    "            bbox_tensor = torch.tensor(bbox, dtype=torch.float32)\n",
    "            num_bboxes = bbox_tensor.size(0)  # Number of bounding boxes\n",
    "\n",
    "            if num_bboxes >= self.max_bboxes:\n",
    "                # Truncate if there are too many bounding boxes\n",
    "                bbox_tensor = bbox_tensor[:self.max_bboxes]\n",
    "            else:\n",
    "                # Pad with zeros if there are too few bounding boxes\n",
    "                pad_size = self.max_bboxes - num_bboxes\n",
    "                padding = torch.zeros((pad_size, 4))\n",
    "                bbox_tensor = torch.cat([bbox_tensor, padding], dim=0)\n",
    "\n",
    "        return bbox_tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image ID\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get image extracted features\n",
    "        feature_vector = self.image_features[img_info['coco_url']]\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(annotations)\n",
    "        \n",
    "        # Initialize an empty mask (will be filled in with object masks), bounding boxes, and category counts\n",
    "        mask = np.zeros((image.size[1], image.size[0]), dtype=np.float32)  # HxW for mask\n",
    "        bounding_boxes = []\n",
    "        category_counts = np.zeros(self.num_classes, dtype=np.float32)  # For counting the number of objects per class\n",
    "\n",
    "        # Loop through annotations and add segmentation masks, bounding boxes, and category counts\n",
    "        for ann in annotations:\n",
    "            if 'segmentation' in ann:\n",
    "                ann_mask = self.coco.annToMask(ann)\n",
    "                mask = np.maximum(mask, ann_mask)  # Combine multiple masks (take max)\n",
    "            # Bounding boxes\n",
    "            if 'bbox' in ann:\n",
    "                bbox = ann['bbox']  # [x, y, width, height]\n",
    "                bounding_boxes.append(bbox)\n",
    "            \n",
    "            # Count the category\n",
    "            category_id = ann['category_id']\n",
    "            category_counts[original_to_mapped[category_id]] += 1  # Categories are 1-indexed in COCO, so subtract 1\n",
    "        \n",
    "        # Pad/truncate bounding boxes\n",
    "        bounding_boxes = self.pad_bounding_boxes(bounding_boxes)\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image, mask, bounding_boxes = self.transform(image, mask, bounding_boxes)\n",
    "            \n",
    "        # Convert to tensor\n",
    "        feature_vector = torch.tensor(feature_vector, dtype=torch.float32)\n",
    "        category_counts = torch.tensor(category_counts, dtype=torch.float32)\n",
    "        bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
    "\n",
    "        return image, mask, bounding_boxes, feature_vector, category_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize image to match ResNet input size\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet statistics\n",
    "# ])\n",
    "\n",
    "class CustomTransform:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.transform_image = transforms.Compose([\n",
    "            transforms.Resize(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet statistics\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image, mask, bounding_boxes):\n",
    "        # Get the original dimensions of the image\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        image = self.transform_image(image)\n",
    "        \n",
    "        # Resize mask (ensure it's the same size as the image)\n",
    "        mask = Image.fromarray(mask)  # Convert mask to PIL Image for resizing\n",
    "        mask = mask.resize((self.size[1], self.size[0]), Image.NEAREST)  # Use nearest neighbor to preserve the mask\n",
    "        mask = np.array(mask)  # Convert back to NumPy array\n",
    "\n",
    "        # Resize bounding boxes based on the new image size\n",
    "        w_ratio = self.size[1] / float(original_width)  # Width resize ratio\n",
    "        h_ratio = self.size[0] / float(original_height)  # Height resize ratio\n",
    "        adjusted_bboxes = []\n",
    "        for bbox in bounding_boxes:\n",
    "            x, y, w, h = bbox\n",
    "            x *= w_ratio\n",
    "            y *= h_ratio\n",
    "            w *= w_ratio\n",
    "            h *= h_ratio\n",
    "            adjusted_bboxes.append([x, y, w, h])\n",
    "\n",
    "        # Convert mask to tensor\n",
    "        mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        # Convert bounding boxes to tensor\n",
    "        adjusted_bboxes = torch.tensor(adjusted_bboxes, dtype=torch.float32)\n",
    "\n",
    "        # Return the transformed image, mask, and bounding boxes\n",
    "        return image, mask, adjusted_bboxes\n",
    "\n",
    "\n",
    "transform = CustomTransform(size=(224, 224))  # Resize to 224x224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=31.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data_type = \"train\"\n",
    "dataset = \"train2017\"\n",
    "\n",
    "# Set your paths\n",
    "train_images_dir = f'data/COCO/{dataset}'\n",
    "train_annotations_dir = f'data/COCO/annotations/instances_{dataset}.json'\n",
    "\n",
    "# Load image feature\n",
    "features_dir = os.path.join(\"data\",\"COCO\",\"extracted_features\",f'{data_type}_image_features.json')\n",
    "with open(features_dir, 'r') as f:\n",
    "    image_features = json.load(f)\n",
    "\n",
    "num_bboxes = 20\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = COCODataset(train_images_dir, train_annotations_dir,image_features,max_bboxes=num_bboxes, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 118287\n",
      "train_loader size: 3697\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset))\n",
    "print(\"train_loader size:\", len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after the last improvement in validation loss.\n",
    "            min_delta (float): Minimum change in validation loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience                  # Number of epochs with no improvement before stopping\n",
    "        self.min_delta = min_delta                # Minimum reduction in validation loss to be considered an improvement\n",
    "        self.best_loss = None                     # Track the best validation loss seen so far\n",
    "        self.counter = 0                          # Count epochs with no improvement\n",
    "        self.early_stop = False                   # Indicator for whether early stopping is triggered\n",
    "        self.best_state = None                    # Store the best model state (parameters)\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if validation loss has improved, and if not, increments the counter.\n",
    "        If the counter exceeds patience, early stopping is triggered.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current validation loss.\n",
    "            model (torch.nn.Module): The model being trained, whose state will be saved if there's an improvement.\n",
    "        \"\"\"\n",
    "        # Update the best loss if this is the lowest loss seen so far\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss             # Update the best loss\n",
    "            self.counter = 0                      # Reset counter since there was improvement\n",
    "            \n",
    "            # Save the model state in memory\n",
    "            self.best_state = model.state_dict()  # Save current model parameters\n",
    "            print(f\"Model state saved with validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            self.counter += 1                     # Increment counter if no improvement\n",
    "            if self.counter >= self.patience:     # Trigger early stopping if patience threshold is met\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, resnet_output_size=2048, max_bboxes=20, num_classes=80, mask_size=224):\n",
    "        \"\"\"\n",
    "        Initialize the multi-modal model.\n",
    "        Args:\n",
    "        - resnet_output_size: The size of the ResNet feature vector (2048 for ResNet-50).\n",
    "        - max_bboxes: Maximum number of bounding boxes (padded/truncated to this size).\n",
    "        - num_classes: Number of output classes (predict counts for each category).\n",
    "        - mask_size: Height/Width of the segmentation mask (assume square mask).\n",
    "        \"\"\"\n",
    "        super(MultiModalModel, self).__init__()\n",
    "\n",
    "        # Image feature processing (ResNet features)\n",
    "        self.resnet_fc = nn.Linear(resnet_output_size, 512)\n",
    "\n",
    "        # Segmentation mask processing\n",
    "        self.mask_conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1)  # Down-sample by factor of 2\n",
    "        self.mask_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)  # Down-sample again\n",
    "        mask_flatten_size = (mask_size // 4) * (mask_size // 4) * 128  # After 2 stride-2 convolutions\n",
    "        self.mask_fc = nn.Linear(mask_flatten_size, 512)\n",
    "\n",
    "        # Bounding box processing\n",
    "        self.bbox_fc1 = nn.Linear(4, 128)  # Process individual bbox coordinates\n",
    "        self.bbox_fc2 = nn.Linear(max_bboxes * 128, 512)  # Process flattened bbox embeddings\n",
    "\n",
    "        # Final count output (predicting counts for each class)\n",
    "        self.fc_count = nn.Sequential(\n",
    "            nn.Linear(512 + 512 + 512, 512),  # Combine features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)  # Output count predictions for each class\n",
    "        )\n",
    "\n",
    "    def forward(self, mask, bbox, feature_vector):\n",
    "        \"\"\"\n",
    "        Forward pass for the multi-modal model.\n",
    "        Args:\n",
    "        - image: Tensor of image data, shape [batch_size, 3, 224, 224] (currently unused in this implementation).\n",
    "        - mask: Tensor of segmentation masks, shape [batch_size, 224, 224].\n",
    "        - bbox: Tensor of bounding boxes, shape [batch_size, max_bboxes, 4].\n",
    "        - feature_vector: Tensor of ResNet features, shape [batch_size, 2048].\n",
    "        \"\"\"\n",
    "        batch_size = feature_vector.size(0)\n",
    "\n",
    "        # ResNet feature vector processing\n",
    "        feature_vector_out = F.relu(self.resnet_fc(feature_vector))  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Mask processing\n",
    "        mask = mask.unsqueeze(1)  # Add channel dimension, shape: [batch_size, 1, 224, 224]\n",
    "        mask_out = F.relu(self.mask_conv1(mask))  # Shape: [batch_size, 64, 112, 112]\n",
    "        mask_out = F.relu(self.mask_conv2(mask_out))  # Shape: [batch_size, 128, 56, 56]\n",
    "        mask_out = mask_out.view(batch_size, -1)  # Flatten, shape: [batch_size, mask_flatten_size]\n",
    "        mask_out = F.relu(self.mask_fc(mask_out))  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Bounding box processing\n",
    "        bbox = bbox.view(-1, 4)  # Flatten bounding boxes across the batch, shape: [batch_size * max_bboxes, 4]\n",
    "        bbox_out = F.relu(self.bbox_fc1(bbox))  # Shape: [batch_size * max_bboxes, 128]\n",
    "        bbox_out = bbox_out.view(batch_size, -1)  # Reshape back to batch size, shape: [batch_size, max_bboxes * 128]\n",
    "        bbox_out = F.relu(self.bbox_fc2(bbox_out))  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Combine all features\n",
    "        combined_features = torch.cat([feature_vector_out, mask_out, bbox_out], dim=1)  # Shape: [batch_size, 512 + 512 + 512]\n",
    "\n",
    "        # Predict counts\n",
    "        count_output = self.fc_count(combined_features)  # Shape: [batch_size, num_classes]\n",
    "\n",
    "        return count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/3697 [00:00<?, ?batch/s]C:\\Users\\pcqm0\\AppData\\Local\\Temp\\ipykernel_15392\\1904055300.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bounding_boxes = torch.tensor(bounding_boxes, dtype=torch.float32)\n",
      "Epoch 1/10: 100%|██████████| 3697/3697 [3:57:31<00:00,  3.85s/batch]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Count Loss: 0.2631\n",
      "Model state saved with validation loss: 0.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3697/3697 [2:44:38<00:00,  2.67s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Count Loss: 0.1874\n",
      "Model state saved with validation loss: 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3697/3697 [3:26:57<00:00,  3.36s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Count Loss: 0.1330\n",
      "Model state saved with validation loss: 0.1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3697/3697 [2:37:14<00:00,  2.55s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Average Count Loss: 0.0865\n",
      "Model state saved with validation loss: 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3697/3697 [2:37:24<00:00,  2.55s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Average Count Loss: 0.0601\n",
      "Model state saved with validation loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3697/3697 [2:38:24<00:00,  2.57s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Average Count Loss: 0.0453\n",
      "Model state saved with validation loss: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3697/3697 [2:41:05<00:00,  2.61s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Average Count Loss: 0.0350\n",
      "Model state saved with validation loss: 0.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3697/3697 [2:47:44<00:00,  2.72s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Average Count Loss: 0.0278\n",
      "Model state saved with validation loss: 0.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3697/3697 [2:49:22<00:00,  2.75s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Average Count Loss: 0.0222\n",
      "Model state saved with validation loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3697/3697 [3:46:03<00:00,  3.67s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Average Count Loss: 0.0178\n",
      "Model state saved with validation loss: 0.0178\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, optimizer, and loss functions\n",
    "num_epochs = 10\n",
    "model = MultiModalModel(resnet_output_size=2048, max_bboxes=num_bboxes, num_classes=80)#.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "early_stopping = EarlyStopping(patience=3)  # Initialize EarlyStopping with patience=3\n",
    "\n",
    "\n",
    "# Loss function: Mean Squared Error for regression\n",
    "criterion_count = nn.MSELoss()  # For predicting counts\n",
    "\n",
    "# Training loop with tqdm for progress bars\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = []\n",
    "\n",
    "    # Use tqdm to iterate through the DataLoader\n",
    "    for images, masks, bounding_boxes, feature_vectors, category_ids in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        # images = images.to(device)\n",
    "        # masks = masks.to(device)\n",
    "        # bounding_boxes = bounding_boxes.to(device)\n",
    "        # feature_vectors = feature_vectors.to(device)\n",
    "        # category_ids = category_ids.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        count_output = model(masks, bounding_boxes, feature_vectors)\n",
    "\n",
    "        # Calculate the count loss (regression)\n",
    "        loss_count = criterion_count(count_output, category_ids.float())  # category_ids should be the true count of each class\n",
    "        \n",
    "        # You may also calculate segmentation and bbox losses if you need them\n",
    "        # loss_segmentation = criterion_segmentation(segmentation_output, masks)\n",
    "        # loss_bbox = criterion_bbox(bbox_output, bounding_boxes)\n",
    "\n",
    "        # Total loss (fusion of regression losses)\n",
    "        total_loss.append(loss_count)  # + loss_segmentation + loss_bbox (if necessary)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_count.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss values for each epoch\n",
    "    avg_loss = sum(total_loss)/len(total_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Average Count Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Check for early stopping and save model if validation loss improves\n",
    "    early_stopping(avg_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model state loaded for testing.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model state after training is finished\n",
    "model.load_state_dict(early_stopping.best_state)\n",
    "print(\"Best model state loaded for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights/countgd-model-weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
