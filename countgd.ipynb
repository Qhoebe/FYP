{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCountingDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotation_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load the annotations (instances_train2017.json)\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "        \n",
    "        self.images = annotations['images']\n",
    "        self.annotations = annotations['annotations']\n",
    "\n",
    "        # Create a mapping for object counts\n",
    "        self.image_to_counts = {}\n",
    "        for ann in self.annotations:\n",
    "            image_id = ann['image_id']\n",
    "            self.image_to_counts[image_id] = self.image_to_counts.get(image_id, 0) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image metadata\n",
    "        image_meta = self.images[idx]\n",
    "        image_id = image_meta['id']\n",
    "        image_path = os.path.join(self.img_dir, image_meta['file_name'])\n",
    "\n",
    "        # Load and transform the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get total object count\n",
    "        total_count = self.image_to_counts.get(image_id, 0)\n",
    "\n",
    "        return image, total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_images_dir = \"data/COCO/train2017\"\n",
    "val_images_dir = \"data/COCO/val2017\"\n",
    "train_annotations = \"data/COCO/annotations/instances_train2017.json\"\n",
    "val_annotations = \"data/COCO/annotations/instances_val2017.json\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CocoCountingDataset(train_images_dir, train_annotations, transform=transform)\n",
    "val_dataset = CocoCountingDataset(val_images_dir, val_annotations, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset size: 118287\n",
      "val_dataset size: 5000\n",
      "train_loader size: 3697\n",
      "val_loader size: 157\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(\"train_dataset size:\", len(train_dataset))\n",
    "print(\"val_dataset size:\", len(val_dataset))\n",
    "print(\"train_loader size:\", len(train_loader))\n",
    "print(\"val_loader size:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectCounterModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObjectCounterModel, self).__init__()\n",
    "        # Use a pre-trained ResNet model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze all layers except for the final fully connected layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False  # Freeze all parameters\n",
    "\n",
    "        # Unfreeze the parameters of the last fully connected layer (fc)\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Replace the final layer for regression (for object counting)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, counts in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        counts = counts.to(device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, counts)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_absolute_error = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, counts in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            counts = counts.to(device, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, counts)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total_absolute_error += torch.sum(torch.abs(outputs - counts)).item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_absolute_error = total_absolute_error / len(dataloader.dataset)\n",
    "    return avg_loss, avg_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Moodswing\\.conda\\envs\\fypEnv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3697 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = ObjectCounterModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_mae = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"object_counter_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path, transform, device):\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict object count\n",
    "    with torch.no_grad():\n",
    "        count = model(image)\n",
    "    return count.item()\n",
    "\n",
    "# Example usage\n",
    "test_image_path = \"data/test2017/example.jpg\"\n",
    "predicted_count = predict(model, test_image_path, transform, device)\n",
    "print(f\"Predicted object count: {predicted_count:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
